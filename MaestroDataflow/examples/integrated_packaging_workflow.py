"""
æ•´åˆç¤ºä¾‹ï¼šç”Ÿäº§æ²»ç†æµç¨‹ï¼ˆæ¸…æ´—â†’å…¥åº“â†’æ ‡å‡†åŒ–æ‰“åŒ…å¹¶è¡¥å……åˆ—æ„ä¹‰ï¼‰

è¿è¡Œæ–¹å¼ï¼š
    python -m examples.integrated_packaging_workflow

è¯´æ˜ï¼š
- ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ DataColumnProcessOperator æ¸…æ´—æ•°æ®ã€å…¥åº“ï¼ˆä¸åœ¨æ­¤é˜¶æ®µç”Ÿæˆåˆ—æ„ä¹‰ï¼‰ã€‚
- ç¬¬äºŒæ­¥ï¼šDatasetPackagingOperator åœ¨æ‰“åŒ…é˜¶æ®µè°ƒç”¨LLMè¡¥å……åˆ—æ„ä¹‰ä¸å•ä½ï¼Œè¾“å‡ºåˆ° output/datasets/ã€‚
- ä¼˜ç‚¹ï¼šæœ‰å®Œæ•´å¤„ç†æŠ¥å‘Šä¸æ•°æ®åº“è½åœ°ï¼Œæ›´é€‚åˆç”Ÿäº§æ²»ç†ã€‚
"""

import os
import json
import re
import pandas as pd
from maestro.utils.db_storage import DBStorage
from maestro.utils.storage import FileStorage
from maestro.operators.data_column_process_ops import DataColumnProcessOperator
from maestro.operators.dataset_ops import DatasetPackagingOperator
from maestro.serving.llm_serving import APILLMServing
from maestro.serving.enhanced_llm_serving import EnhancedLLMServing


def setup_llm_service():
    """ä½¿ç”¨ DeepSeek API ä½œä¸ºLLMæœåŠ¡ã€‚"""
    api_key = os.getenv("DEEPSEEK_API_KEY", "")
    if not api_key:
        print("âš ï¸ æœªè®¾ç½®DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†æŒ‰æ¼”ç¤ºç»§ç»­ä½†å¯èƒ½è¿”å›å ä½è¯´æ˜")
        api_key = "demo-key-placeholder"
    base_serving = APILLMServing(
        api_url="https://api.deepseek.com/v1/chat/completions",
        api_key=api_key,
        model_name="deepseek-chat",
        api_type="openai"
    )
    service = EnhancedLLMServing(base_serving=base_serving, enable_cache=True)
    print("âœ… ä½¿ç”¨DeepSeek API LLMæœåŠ¡")
    return service

 


def main():
    # è¾“å…¥æ•°æ®ï¼šä¼˜å…ˆè‡ªåŠ¨ä» input/ ç›®å½•æŸ¥æ‰¾ .xlsx/.csv æ–‡ä»¶
    def find_input_file() -> str:
        # ä»…è¯»å– input/datasets/ ç›®å½•
        search_root = os.path.join(os.getcwd(), "input", "datasets")
        # 1) å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šï¼Œä½†å¿…é¡»ä½äº input/datasets/
        env_path = os.getenv("MAESTRO_INPUT_FILE")
        if env_path and os.path.exists(env_path):
            abs_env = os.path.abspath(env_path)
            abs_root = os.path.abspath(search_root)
            if abs_env.startswith(abs_root):
                print(f"ğŸ” ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„è¾“å…¥æ–‡ä»¶: {env_path}")
                return env_path
            else:
                print("âš ï¸ MAESTRO_INPUT_FILE æœªä½äº input/datasets/ ä¸‹ï¼Œå·²å¿½ç•¥è¯¥è®¾ç½®")
        # 2) é€’å½’æœç´¢ input/datasets/ ç›®å½•ä¼˜å…ˆ .xlsx, å…¶æ¬¡ .csv
        candidates_xlsx = []
        candidates_csv = []
        if os.path.isdir(search_root):
            for root, _, files in os.walk(search_root):
                for name in files:
                    # æ’é™¤ä¸´æ—¶/éšè—æ–‡ä»¶ï¼ˆå¦‚ Excel çš„ ~$ å‰ç¼€ã€. å¼€å¤´ç­‰ï¼‰
                    if name.startswith('~$') or name.startswith('.') or name.startswith('._'):
                        continue
                    lower = name.lower()
                    path = os.path.join(root, name)
                    if lower.endswith('.xlsx'):
                        candidates_xlsx.append(path)
                    elif lower.endswith('.csv'):
                        candidates_csv.append(path)
        # 3) é€‰æ‹©ä¼˜å…ˆé¡¹
        if candidates_xlsx:
            chosen = sorted(candidates_xlsx)[0]
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°è¾“å…¥xlsx: {chosen}")
            return chosen
        if candidates_csv:
            chosen = sorted(candidates_csv)[0]
            print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°è¾“å…¥csv: {chosen}")
            return chosen
        # 4) æ˜ç¡®é”™è¯¯æç¤ºï¼Œä»…é™ input/datasets/
        raise FileNotFoundError(
            "æœªåœ¨ input/datasets/ ç›®å½•æ‰¾åˆ° .xlsx æˆ– .csv æ–‡ä»¶ã€‚",
        )

    input_path = find_input_file()

    # å­˜å‚¨åˆå§‹åŒ–
    storage = FileStorage(
        input_file_path=input_path,
        cache_path="./output/integrated_packaging/cache",
        file_name_prefix="integrated",
        cache_type="csv"
    )

    # LLMæœåŠ¡
    llm_service = setup_llm_service()

    # ç¬¬ä¸€æ­¥ï¼šæ•´åˆå¤„ç†ï¼ˆæ¸…æ´—â†’å…¥åº“â†’LLMåˆ—æ„ä¹‰ï¼‰
    # æ•°æ®é›†åç§°æ”¹ä¸ºæºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    base_name = os.path.splitext(os.path.basename(input_path))[0]
    dataset_name = base_name
    dataset_description = f"{base_name} æ•°æ®é›†ï¼Œè‡ªåŠ¨æ‰“åŒ…å¹¶è¡¥å……åˆ—æ„ä¹‰ä¸å•ä½ï¼Œé€‚ç”¨äºç”Ÿäº§æ²»ç†æµç¨‹ã€‚"

    # ç”Ÿæˆè‹±æ–‡ç®€ç§°å¹¶æ„å»ºç±»åï¼Œç”¨äºæ•°æ®åº“è¡¨å
    def _slugify(text: str) -> str:
        slug = re.sub(r"[^A-Za-z0-9]+", "", text)
        return slug or "Dataset"

    def _shorten_slug(slug: str) -> str:
        letters_only = re.sub(r"[^A-Za-z]", "", slug)
        if not letters_only:
            return "Dataset"
        acronym = "".join(c for c in letters_only if c.isupper())
        if len(acronym) >= 3:
            return acronym
        short = letters_only[:10]
        return (short[0].upper() + short[1:]) if short else "Dataset"

    ascii_slug = _slugify(base_name)
    # è‹¥å»é™¤éASCIIåæ²¡æœ‰å­—æ¯ï¼ˆå¯èƒ½ä»…æœ‰æ•°å­—æˆ–ä¸­æ–‡ï¼‰ï¼Œå°è¯•ç”¨LLMç”Ÿæˆè‹±æ–‡ç®€ç§°
    if not re.search(r"[A-Za-z]", ascii_slug) and llm_service is not None:
        try:
            prompt = (
                f"Generate a concise English abbreviation (letters only, PascalCase) for the dataset name '{base_name}'. "
                f"Return ONLY the abbreviation without any explanations."
            )
            resp = llm_service.generate(prompt)
            candidate = re.sub(r"[^A-Za-z]+", "", resp).strip()
            if candidate:
                ascii_slug = candidate
        except Exception as e:
            print(f"è­¦å‘Šï¼šè‹±æ–‡ç®€ç§°ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å›é€€ã€‚é”™è¯¯: {e}")
    short_slug = _shorten_slug(ascii_slug)
    class_name_for_table = f"Dataset{short_slug}"
    db_path = "output/integrated_packaging/maestro_data.db"
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
    db_conn = f"sqlite:///{db_path}"

    processor = DataColumnProcessOperator(
        dataset_name=dataset_name,
        dataset_description=dataset_description,
        db_connection_string=db_conn,
        table_name=class_name_for_table,
        service=llm_service
    )

    # æå‰åˆ›å»ºæ•°æ®é›†ç›®å½•ï¼Œå¹¶è¾“å‡ºåŸå§‹åˆ—åJSONï¼ˆä¸å«æ„ä¹‰ä¸å•ä½ï¼‰
    output_root = "output/datasets"
    dataset_dir = os.path.join(output_root, dataset_name)
    os.makedirs(dataset_dir, exist_ok=True)
    try:
        # FileStorageéœ€è¦å…ˆstep()åˆå§‹åŒ–å¤„ç†æ­¥éª¤ï¼Œstep(0)è¯»å–åŸå§‹è¾“å…¥æ–‡ä»¶
        raw_df = storage.step().read(output_type="dataframe")
        raw_columns = list(map(str, raw_df.columns))
        ori_meanings = {col: {"æ„ä¹‰": "", "å•ä½": ""} for col in raw_columns}
        ori_path = os.path.join(dataset_dir, "all_column_name_ori.json")
        with open(ori_path, "w", encoding="utf-8") as f:
            json.dump(ori_meanings, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ å·²è¾“å‡ºåŸå§‹åˆ—åJSON: {ori_path}")
    except Exception as e:
        print(f"âš ï¸ è¯»å–åŸå§‹æ•°æ®åˆ—åå¤±è´¥ï¼Œè·³è¿‡è¾“å‡º all_column_name_ori.json: {e}")

    # å°è¯•è¿è¡Œæ•´åˆå¤„ç†ï¼›å¦‚LLMä¸å¯ç”¨åˆ™å›é€€å ä½è¯´æ˜
    try:
        process_result = processor.run(
            storage=storage,
            na_threshold=0.3,
            fill_method="median",
            llm_service=llm_service
        )

        # ä¸åœ¨æ¸…æ´—é˜¶æ®µç”Ÿæˆåˆ—æ„ä¹‰æ˜ å°„ï¼Œæ”¹ä¸ºåœ¨æ‰“åŒ…é˜¶æ®µè°ƒç”¨LLMè¡¥å……
        meanings_mapping = None
    except Exception as e:
        print(f"âš ï¸ æ•´åˆå¤„ç†å¤±è´¥ï¼Œå°†åœ¨æ‰“åŒ…é˜¶æ®µè°ƒç”¨LLMè¡¥å……åˆ—æ„ä¹‰: {e}")
        meanings_mapping = None

    print("\nğŸ”§ æ¸…æ´—ä¸å…¥åº“å®Œæˆï¼Œå‡†å¤‡æ ‡å‡†åŒ–æ‰“åŒ…å¹¶è¡¥å……åˆ—æ„ä¹‰...")

    # ç›´æ¥æ²¿ç”¨å½“å‰å¤„ç†ä¸Šä¸‹æ–‡ï¼Œè®©æ‰“åŒ…è¯»å–åˆ°æœ€æ–°ç¼“å­˜ï¼ˆé¿å…æ‰¾ä¸åˆ°ä¸Šä¸€æ­¥ç”Ÿæˆçš„CSVï¼‰

    # ç¬¬äºŒæ­¥ï¼šæ ‡å‡†åŒ–æ‰“åŒ…ï¼ˆåœ¨æ‰“åŒ…é˜¶æ®µè°ƒç”¨LLMè¡¥å……åˆ—æ„ä¹‰ï¼‰
    # ä¼˜å…ˆä»æ•°æ®åº“è¯»å–æ¸…æ´—åçš„DataFrameï¼ˆç”±å¤„ç†é˜¶æ®µå†™å…¥ 'cleaned_data'ï¼‰
    cleaned_df = None
    try:
        table_name = class_name_for_table
        db_reader = DBStorage(connection_string=db_conn, table_name=table_name)
        db_reader.step_count = 1  # å¤„ç†é˜¶æ®µå†™å…¥ä½¿ç”¨äº† step=1
        cleaned_df = db_reader.read(output_type="dataframe", key="cleaned_data")
        if isinstance(cleaned_df, pd.DataFrame) and not cleaned_df.empty:
            print(f"ğŸ“¦ å·²ä»æ•°æ®åº“è¯»å–æ¸…æ´—åçš„DataFrameç”¨äºæ‰“åŒ…: è¡¨ {table_name}, step 1, key 'cleaned_data'")
    except Exception as e:
        print(f"âš ï¸ ä»æ•°æ®åº“è¯»å–æ¸…æ´—åæ•°æ®å¤±è´¥ï¼Œå°†å°è¯•ä½¿ç”¨ç¼“å­˜å›é€€: {e}")

    # å›é€€ï¼šä»ç¼“å­˜ç›®å½•æŒ‘é€‰æœ€æ–°çš„CSVï¼ˆå¯èƒ½æ˜¯ä¸­é—´äº§ç‰©ï¼Œå°½é‡ä½œä¸ºå¤‡ç”¨ï¼‰
    if cleaned_df is None or cleaned_df.empty:
        try:
            cache_dir = getattr(storage, "cache_path", None)
            prefix = getattr(storage, "file_name_prefix", None)
            if cache_dir and prefix and os.path.isdir(cache_dir):
                files = [f for f in os.listdir(cache_dir) if f.startswith(prefix+"_") and f.endswith(".csv")]
                def _suffix_num(name: str) -> int:
                    try:
                        base = os.path.splitext(name)[0]
                        return int(base.split("_")[-1])
                    except:
                        return -1
                files_sorted = sorted(files, key=_suffix_num, reverse=True)
                if files_sorted:
                    latest_path = os.path.join(cache_dir, files_sorted[0])
                    cleaned_df = pd.read_csv(latest_path)
                    print(f"ğŸ“¦ è¯»å–ç¼“å­˜DataFrameç”¨äºæ‰“åŒ…: {latest_path}")
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç¼“å­˜DataFrameå¤±è´¥ï¼Œå°†ä½¿ç”¨åŸstorageè¯»å–: {e}")

    # æ„é€ ä¸€ä¸ªæœ€å°å­˜å‚¨åŒ…è£…ï¼Œæ”¯æŒ step/read/writeï¼Œä¾›æ‰“åŒ…ä¸LLMç®—å­ä½¿ç”¨
    class _DFStorage:
        def __init__(self, df: pd.DataFrame):
            self._df = df
            self.operator_step = -1
        def step(self):
            self.operator_step += 1
            return self
        def write(self, data, **kwargs):
            if isinstance(data, pd.DataFrame):
                self._df = data
            return "memory://df"
        def read(self, output_type="dataframe", **kwargs):
            return self._df

    effective_storage = _DFStorage(cleaned_df) if isinstance(cleaned_df, pd.DataFrame) else storage

    packer = DatasetPackagingOperator(dataset_name=dataset_name)
    package_result = packer.run(
        storage=effective_storage,
        service=llm_service,
        output_root=output_root,
        dataset_description=dataset_description,
        meanings_mapping=meanings_mapping,
        # ä¼ é€’æºxlsxæ–‡ä»¶åç”¨äºç”Ÿæˆè‹±æ–‡ç®€ç§°
        slug_source=input_path
    )

    print("\nâœ… ç”Ÿäº§æ²»ç†æ•´åˆæµç¨‹å®Œæˆï¼š")
    for k, v in package_result.items():
        print(f"- {k}: {v}")


if __name__ == "__main__":
    main()
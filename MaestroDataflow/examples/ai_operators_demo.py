"""
MaestroDataflow AI Operators Demo
æ¼”ç¤ºå„ç§AIæ“ä½œç¬¦çš„ä½¿ç”¨æ–¹æ³•å’ŒåŠŸèƒ½
"""

import pandas as pd
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯¼å…¥MaestroDataflowç»„ä»¶
from maestro.pipeline.pipeline import Pipeline
from maestro.utils.storage import FileStorage
from maestro.serving.enhanced_llm_serving import EnhancedLLMServing, LocalLLMServing
from maestro.serving.llm_serving import APILLMServing
from maestro.core.prompt import DIYPromptABC

# å¯¼å…¥AIæ“ä½œç¬¦
from maestro.operators.ai_ops import (
    PromptedGenerator, TextSummarizer, TextClassifier,
    EmbeddingGenerator, SimilarityCalculator, TextMatcher,
    KnowledgeBaseBuilder, RAGRetriever, RAGOperator,
    ImageProcessor, AudioProcessor, VideoProcessor, MultimodalFusion,
    AutoDataCleaner, SmartAnnotator, FeatureEngineer
)


def setup_demo_environment():
    """è®¾ç½®æ¼”ç¤ºç¯å¢ƒ"""
    print("ğŸš€ è®¾ç½®MaestroDataflow AIæ“ä½œç¬¦æ¼”ç¤ºç¯å¢ƒ...")

    # åˆ›å»ºå­˜å‚¨å®ä¾‹
    storage = FileStorage(
        input_file_path="../sample_data/employees.csv",
        cache_path="../output/ai_operators_demo/cache",
        file_name_prefix="demo_cache",
        cache_type="csv",
        enable_vector_storage=True,
        enable_model_cache=True
    )

    # åˆ›å»ºLLMæœåŠ¡å®ä¾‹ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–APIï¼‰
    try:
        # å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        llm_serving = LocalLLMServing(
            model_name="microsoft/DialoGPT-medium",
            device="cpu"
        )
        print("âœ… ä½¿ç”¨æœ¬åœ°LLMæ¨¡å‹")
    except Exception as e:
        # å›é€€åˆ°APIæœåŠ¡
        import os
        api_key = os.getenv("OPENAI_API_KEY", "")
        if not api_key:
            print("âš ï¸ æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œè¯·è®¾ç½®åä½¿ç”¨APIæœåŠ¡")
            api_key = "demo-key-placeholder"  # ä»…ç”¨äºæ¼”ç¤ºï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®APIå¯†é’¥
        
        # åˆ›å»ºåŸºç¡€APIæœåŠ¡
        base_serving = APILLMServing(
            api_url="https://api.openai.com/v1",
            api_key=api_key,
            model_name="gpt-3.5-turbo",
            api_type="openai"
        )
        
        # åˆ›å»ºå¢å¼ºæœåŠ¡
        llm_serving = EnhancedLLMServing(
            base_serving=base_serving,
            enable_cache=True
        )
        print("âœ… ä½¿ç”¨API LLMæœåŠ¡")

    return storage, llm_serving


def demo_text_generation_operators(storage, llm_serving):
    """æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆæ“ä½œç¬¦"""
    print("\nğŸ“ === æ–‡æœ¬ç”Ÿæˆæ“ä½œç¬¦æ¼”ç¤º ===")

    # å‡†å¤‡ç¤ºä¾‹æ•°æ®
    sample_texts = [
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œï¼Œä»è‡ªåŠ¨é©¾é©¶æ±½è½¦åˆ°æ™ºèƒ½åŠ©æ‰‹ï¼ŒAIæŠ€æœ¯æ— å¤„ä¸åœ¨ã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ã€‚",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ï¼Œåœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚"
    ]

    df = pd.DataFrame({"text": sample_texts})
    storage.write(df)

    # 1. æç¤ºè¯ç”Ÿæˆå™¨æ¼”ç¤º
    print("\n1ï¸âƒ£ æç¤ºè¯ç”Ÿæˆå™¨æ¼”ç¤º")
    prompt_generator = PromptedGenerator(
        llm_serving=llm_serving,
        prompt=DIYPromptABC("è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªåˆ›æ„æ ‡é¢˜ï¼š{text}"),
        input_column="text"
    )

    result = prompt_generator.run(storage, input_path="sample_texts", output_path="generated_titles")
    print(f"ç”Ÿæˆç»“æœ: æˆåŠŸç”Ÿæˆ {result['generated_count']} ä¸ªæ ‡é¢˜")

    # 2. æ–‡æœ¬æ‘˜è¦å™¨æ¼”ç¤º
    print("\n2ï¸âƒ£ æ–‡æœ¬æ‘˜è¦å™¨æ¼”ç¤º")
    summarizer = TextSummarizer(
        llm_serving=llm_serving,
        input_column="text",
        max_length=50
    )

    result = summarizer.run(storage, input_path="sample_texts", output_path="summaries")
    print(f"æ‘˜è¦ç»“æœ: æˆåŠŸç”Ÿæˆ {result['summarized_count']} ä¸ªæ‘˜è¦")

    # 3. æ–‡æœ¬åˆ†ç±»å™¨æ¼”ç¤º
    print("\n3ï¸âƒ£ æ–‡æœ¬åˆ†ç±»å™¨æ¼”ç¤º")
    classifier = TextClassifier(
        llm_serving=llm_serving,
        input_column="text",
        categories=["æŠ€æœ¯", "ç§‘å­¦", "æ•™è‚²"]
    )

    result = classifier.run(storage, input_path="sample_texts", output_path="classifications")
    print(f"åˆ†ç±»ç»“æœ: æˆåŠŸåˆ†ç±» {result['classified_count']} ä¸ªæ–‡æœ¬")


def demo_embedding_operators(storage, llm_serving):
    """æ¼”ç¤ºåµŒå…¥å‘é‡æ“ä½œç¬¦"""
    print("\nğŸ” === åµŒå…¥å‘é‡æ“ä½œç¬¦æ¼”ç¤º ===")

    # å‡†å¤‡æŸ¥è¯¢æ•°æ®
    queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ å¦‚ä½•å·¥ä½œï¼Ÿ",
        "æ·±åº¦å­¦ä¹ çš„åº”ç”¨é¢†åŸŸ"
    ]

    df_queries = pd.DataFrame({"query": queries})
    storage.write(df_queries)

    # 1. åµŒå…¥ç”Ÿæˆå™¨æ¼”ç¤º
    print("\n1ï¸âƒ£ åµŒå…¥ç”Ÿæˆå™¨æ¼”ç¤º")
    embedding_generator = EmbeddingGenerator(
        input_column="query",
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        device="cpu"
    )

    result = embedding_generator.run(storage.step())
    print(f"åµŒå…¥ç”Ÿæˆç»“æœ: æˆåŠŸç”Ÿæˆ {result['embedded_count']} ä¸ªåµŒå…¥å‘é‡")

    # 2. ç›¸ä¼¼åº¦è®¡ç®—å™¨æ¼”ç¤º
    print("\n2ï¸âƒ£ ç›¸ä¼¼åº¦è®¡ç®—å™¨æ¼”ç¤º")
    similarity_calculator = SimilarityCalculator(
        embedding_column="embedding",
        reference_texts=["æŠ€æœ¯æ–‡æ¡£", "ç§‘å­¦ç ”ç©¶", "æ•™è‚²èµ„æ–™"],
        similarity_metric="cosine",
        embedding_model="sentence-transformers/all-MiniLM-L6-v2"
    )

    result = similarity_calculator.run(storage.step())
    print(f"ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ: æˆåŠŸè®¡ç®— {result['calculated_count']} ä¸ªç›¸ä¼¼åº¦")

    # 3. æ–‡æœ¬åŒ¹é…å™¨æ¼”ç¤º
    print("\n3ï¸âƒ£ æ–‡æœ¬åŒ¹é…å™¨æ¼”ç¤º")
    reference_texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "æœºå™¨å­¦ä¹ ä½¿ç”¨ç®—æ³•æ¥åˆ†ææ•°æ®",
        "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä¸­å¾ˆæœ‰ç”¨"
    ]

    text_matcher = TextMatcher(
        input_column="query",
        reference_texts=reference_texts,
        similarity_threshold=0.5
    )

    result = text_matcher.run(storage)
    print(f"æ–‡æœ¬åŒ¹é…ç»“æœ: æˆåŠŸåŒ¹é… {result['matched_count']} ä¸ªæ–‡æœ¬")


def demo_rag_operators(storage, llm_serving):
    """æ¼”ç¤ºRAGæ“ä½œç¬¦"""
    print("\nğŸ§  === RAGæ“ä½œç¬¦æ¼”ç¤º ===")

    # å‡†å¤‡çŸ¥è¯†åº“æ–‡æ¡£
    documents = [
        "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯æŒ‡ç”±æœºå™¨å±•ç°å‡ºçš„æ™ºèƒ½ï¼Œä¸äººç±»å’ŒåŠ¨ç‰©å±•ç°çš„è‡ªç„¶æ™ºèƒ½å½¢æˆå¯¹æ¯”ã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨ç»Ÿè®¡æŠ€æœ¯è®©è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚",
        "è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œè‡´åŠ›äºè®©æœºå™¨èƒ½å¤Ÿç†è§£å’Œè§£é‡Šè§†è§‰ä¿¡æ¯ã€‚"
    ]

    df_docs = pd.DataFrame({"document": documents})
    storage.write(df_docs)

    # 1. çŸ¥è¯†åº“æ„å»ºå™¨æ¼”ç¤º
    print("\n1ï¸âƒ£ çŸ¥è¯†åº“æ„å»ºå™¨æ¼”ç¤º")
    kb_builder = KnowledgeBaseBuilder(
        text_column="text",
        chunk_size=200,
        chunk_overlap=50
    )

    result = kb_builder.run(storage, input_path="knowledge_documents", output_path="knowledge_base")
    print(f"çŸ¥è¯†åº“æ„å»ºç»“æœ: æˆåŠŸæ„å»º {result['total_chunks']} ä¸ªçŸ¥è¯†å—")

    # 2. RAGæ£€ç´¢å™¨æ¼”ç¤º
    print("\n2ï¸âƒ£ RAGæ£€ç´¢å™¨æ¼”ç¤º")
    rag_retriever = RAGRetriever(
        query_column="query",
        top_k=3,
        similarity_threshold=0.3
    )

    # å‡†å¤‡æŸ¥è¯¢
    query_data = pd.DataFrame({"query": ["ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"]})
    storage.write(query_data)

    result = rag_retriever.run(
        storage,
        query_path="rag_queries",
        knowledge_base_path="knowledge_base",
        output_path="retrieved_docs"
    )
    print(f"RAGæ£€ç´¢ç»“æœ: æˆåŠŸæ£€ç´¢ {result['total_retrieved']} ä¸ªæ–‡æ¡£")

    # 3. RAGæ“ä½œç¬¦æ¼”ç¤º
    print("\n3ï¸âƒ£ RAGæ“ä½œç¬¦æ¼”ç¤º")
    rag_operator = RAGOperator(
        llm_serving=llm_serving,
        max_context_length=500,
        include_sources=True
    )

    result = rag_operator.run(storage.step())
    print(f"RAGç”Ÿæˆç»“æœ: æˆåŠŸå¤„ç† {result['successful_responses']} ä¸ªæŸ¥è¯¢")


def demo_intelligent_processing_operators(storage, llm_serving):
    """æ¼”ç¤ºæ™ºèƒ½æ•°æ®å¤„ç†æ“ä½œç¬¦"""
    print("\nğŸ¤– === æ™ºèƒ½æ•°æ®å¤„ç†æ“ä½œç¬¦æ¼”ç¤º ===")

    # å‡†å¤‡éœ€è¦æ¸…æ´—çš„æ•°æ®
    dirty_data = pd.DataFrame({
        "name": ["å¼ ä¸‰", "æå››", "ç‹äº”", "å¼ ä¸‰", "èµµå…­", None, "é’±ä¸ƒ"],
        "email": ["zhang@email.com", "li@email", "wang@email.com", "zhang@email.com", "zhao@email.com", "", "qian@email.com"],
        "age": [25, 30, None, 25, 35, 28, 40],
        "score": [85.5, 92.0, 78.5, 85.5, 88.0, 95.0, 72.0],
        "comment": ["å¾ˆå¥½çš„äº§å“", "è´¨é‡ä¸é”™", "è¿˜å¯ä»¥", "å¾ˆå¥½çš„äº§å“", "éå¸¸æ»¡æ„", "ä¸€èˆ¬èˆ¬", "éœ€è¦æ”¹è¿›"]
    })

    storage.write(dirty_data)

    # 1. è‡ªåŠ¨æ•°æ®æ¸…æ´—å™¨æ¼”ç¤º
    print("\n1ï¸âƒ£ è‡ªåŠ¨æ•°æ®æ¸…æ´—å™¨æ¼”ç¤º")
    data_cleaner = AutoDataCleaner(
        llm_serving=llm_serving,
        cleaning_strategies=["remove_duplicates", "handle_missing", "standardize_format"],
        confidence_threshold=0.8
    )

    result = data_cleaner.run(storage, input_path="dirty_data", output_path="cleaned_data")
    print(f"æ•°æ®æ¸…æ´—ç»“æœ: æ¸…æ´—äº† {result['final_shape'][0]} æ¡è®°å½•")
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {result['original_shape']}, æ¸…æ´—åå½¢çŠ¶: {result['final_shape']}")

    # 2. æ™ºèƒ½æ ‡æ³¨å™¨æ¼”ç¤º
    print("\n2ï¸âƒ£ æ™ºèƒ½æ ‡æ³¨å™¨æ¼”ç¤º")
    annotator = SmartAnnotator(
        llm_serving=llm_serving,
        annotation_type="sentiment",
        target_column="comment",
        output_column="sentiment"
    )

    result = annotator.run(storage, input_path="cleaned_data", output_path="annotated_data")
    print(f"æ™ºèƒ½æ ‡æ³¨ç»“æœ: æ ‡æ³¨äº† {result['annotated_count']} æ¡è®°å½•")
    print(f"æ ‡æ³¨ç»Ÿè®¡: {result.get('annotation_stats', {})}")

    # 3. ç‰¹å¾å·¥ç¨‹å™¨æ¼”ç¤º
    print("\n3ï¸âƒ£ ç‰¹å¾å·¥ç¨‹å™¨æ¼”ç¤º")
    feature_engineer = FeatureEngineer(
        llm_serving=llm_serving,
        feature_types=["statistical", "text", "categorical"],
        max_features=20
    )

    result = feature_engineer.run(storage, input_path="annotated_data", output_path="engineered_features")
    print(f"ç‰¹å¾å·¥ç¨‹ç»“æœ: ç”Ÿæˆäº† {result['final_feature_count']} ä¸ªç‰¹å¾")
    print(f"åŸå§‹ç‰¹å¾æ•°: {result['original_feature_count']}, æœ€ç»ˆç‰¹å¾æ•°: {result['final_feature_count']}")


def demo_multimodal_operators(storage, llm_serving):
    """æ¼”ç¤ºå¤šæ¨¡æ€æ“ä½œç¬¦"""
    print("\nğŸ¨ === å¤šæ¨¡æ€æ“ä½œç¬¦æ¼”ç¤º ===")

    # å¤šæ¨¡æ€å¤„ç†ç¤ºä¾‹
    print("\n=== å¤šæ¨¡æ€å¤„ç†ç¤ºä¾‹ ===")
    
    # å›¾åƒå¤„ç†
    image_processor = ImageProcessor(
        llm_serving=llm_serving,
        task_type="describe",
        image_column="image_path",
        output_column="image_description"
    )
    result = image_processor.run(storage, input_path="sample_images.csv", output_path="image_descriptions.csv")
    print(f"å›¾åƒå¤„ç†å®Œæˆï¼Œå¤„ç†äº† {result['processed_count']} å¼ å›¾åƒ")
    
    # éŸ³é¢‘å¤„ç†
    audio_processor = AudioProcessor(
        llm_serving=llm_serving,
        task_type="transcribe",
        audio_column="audio_path",
        output_column="transcription"
    )
    result = audio_processor.run(storage, input_path="sample_audios.csv", output_path="audio_transcriptions.csv")
    print(f"éŸ³é¢‘å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {result['processed_count']} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    
    # è§†é¢‘å¤„ç†
    video_processor = VideoProcessor(
        llm_serving=llm_serving,
        task_type="analyze",
        video_column="video_path",
        output_column="video_analysis"
    )
    result = video_processor.run(storage, input_path="sample_videos.csv", output_path="video_analyses.csv")
    print(f"è§†é¢‘å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {result['processed_count']} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    # å¤šæ¨¡æ€èåˆ
    multimodal_fusion = MultimodalFusion(
        llm_serving=llm_serving,
        modalities=["text", "image"],
        fusion_strategy="concatenate",
        output_column="multimodal_analysis"
    )
    result = multimodal_fusion.run(storage, input_path="multimodal_data.csv", output_path="multimodal_results.csv")
    print(f"å¤šæ¨¡æ€èåˆå®Œæˆï¼Œå¤„ç†äº† {result['processed_count']} æ¡è®°å½•")


def demo_workflow_integration():
    """æ¼”ç¤ºå·¥ä½œæµé›†æˆ"""
    print("\nğŸ”„ === å·¥ä½œæµé›†æˆæ¼”ç¤º ===")

    # åˆ›å»ºç®¡é“
    pipeline = Pipeline("AI_Processing_Pipeline")

    # è®¾ç½®å­˜å‚¨å’ŒLLMæœåŠ¡
    storage, llm_serving = setup_demo_environment()

    # åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„AIå¤„ç†ç®¡é“
    print("åˆ›å»ºAIå¤„ç†ç®¡é“...")

    # æ­¥éª¤1: æ•°æ®æ¸…æ´—
    data_cleaner = AutoDataCleaner(llm_serving=llm_serving)

    # æ­¥éª¤2: æ–‡æœ¬åˆ†ç±»
    classifier = TextClassifier(
        llm_serving=llm_serving,
        input_column="text",
        categories=["æ­£é¢", "è´Ÿé¢", "ä¸­æ€§"]
    )

    # æ­¥éª¤3: ç‰¹å¾å·¥ç¨‹
    feature_engineer = FeatureEngineer(llm_serving=llm_serving)

    print("âœ… AIå¤„ç†ç®¡é“åˆ›å»ºå®Œæˆ")
    print("ç®¡é“åŒ…å«: æ•°æ®æ¸…æ´— â†’ æ–‡æœ¬åˆ†ç±» â†’ ç‰¹å¾å·¥ç¨‹")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ¯ MaestroDataflow AIæ“ä½œç¬¦ç»¼åˆæ¼”ç¤º")
    print("=" * 50)

    try:
        # è®¾ç½®ç¯å¢ƒ
        storage, llm_serving = setup_demo_environment()

        # è¿è¡Œå„ä¸ªæ¼”ç¤º
        demo_text_generation_operators(storage, llm_serving)
        demo_embedding_operators(storage, llm_serving)
        demo_rag_operators(storage, llm_serving)
        demo_intelligent_processing_operators(storage, llm_serving)
        demo_multimodal_operators(storage, llm_serving)
        demo_workflow_integration()

        print("\nğŸ‰ === æ¼”ç¤ºå®Œæˆ ===")
        print("æ‰€æœ‰AIæ“ä½œç¬¦æ¼”ç¤ºå·²æˆåŠŸè¿è¡Œï¼")
        print("è¯·æŸ¥çœ‹ ./demo_data ç›®å½•ä¸­çš„è¾“å‡ºæ–‡ä»¶ã€‚")

        # æ˜¾ç¤ºå­˜å‚¨ç»Ÿè®¡
        if hasattr(storage, 'get_cache_stats'):
            cache_stats = storage.get_cache_stats()
            print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡: {cache_stats}")

        if hasattr(storage, 'get_vector_stats'):
            vector_stats = storage.get_vector_stats()
            print(f"ğŸ“Š å‘é‡å­˜å‚¨ç»Ÿè®¡: {vector_stats}")

    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–é¡¹æ˜¯å¦æ­£ç¡®å®‰è£…ã€‚")


if __name__ == "__main__":
    main()
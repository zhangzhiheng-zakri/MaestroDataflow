"""
æ•´åˆçš„åˆ—åå¤„ç†å·¥ä½œæµç¨‹ç¤ºä¾‹


å®Œæ•´çš„ä¸‰æ­¥éª¤å·¥ä½œæµç¨‹ï¼š
1. ç¬¬ä¸€æ­¥ï¼šç”ŸæˆåŒ…å«ç©ºæ„ä¹‰å­—æ®µçš„JSONæ¨¡æ¿ (è¾“å‡ºå…¨éƒ¨åˆ—å)
2. ç¬¬äºŒæ­¥ï¼šè¿›è¡Œæ•°æ®å¤„ç† (æ•°æ®æ¸…æ´—ã€å­˜å‚¨åˆ°æ•°æ®åº“)
3. ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨å¤§æ¨¡å‹å¡«å……JSONçš„æ„ä¹‰å­—æ®µ

è¿™ä¸ªå·¥ä½œæµç¨‹æ»¡è¶³ç”¨æˆ·çš„éœ€æ±‚ï¼šå…ˆè¾“å‡ºå…¨éƒ¨åˆ—åï¼Œç„¶åè¿›è¡Œæ•°æ®å¤„ç†ï¼Œæœ€åè¿›è¡Œå¤§æ¨¡å‹å¯¹JSONçš„å¡«å……
"""

import os
import sys
import json
import pandas as pd
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from maestro.utils.storage import FileStorage
from maestro.pipeline import Pipeline
from maestro.operators.column_template_ops import ColumnTemplateGeneratorOperator
from maestro.operators.column_ops import ColumnMeaningGeneratorOperator
from maestro.operators.data_column_process_ops import DataColumnProcessOperator
from maestro.operators.io_ops import LoadDataOperator


# åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ‹ŸLLMæœåŠ¡
class MockLLMService:
    def generate(self, prompt: str, **kwargs) -> str:
        """æ¨¡æ‹ŸLLMå“åº”ï¼Œè¿”å›JSONæ ¼å¼çš„åˆ—åè§£é‡Š"""
        # æ ¹æ®promptä¸­çš„åˆ—åè¿”å›ç›¸åº”çš„JSONæ ¼å¼å“åº”
        
        # ç”¨æˆ·è¡Œä¸ºæ•°æ®ç›¸å…³åˆ—åçš„ä¸“ä¸šè§£é‡Š
        column_mappings = {
            "age": {"æ„ä¹‰": "ç”¨æˆ·çš„å¹´é¾„ï¼Œè¡¨ç¤ºä»å‡ºç”Ÿåˆ°å½“å‰æ—¶é—´çš„å¹´æ•°ï¼Œæ˜¯é‡è¦çš„äººå£ç»Ÿè®¡å­¦å˜é‡ï¼Œç”¨äºåˆ†æä¸åŒå¹´é¾„æ®µç”¨æˆ·çš„è¡Œä¸ºç‰¹å¾å’Œåå¥½å·®å¼‚ã€‚", "å•ä½": "å¹´"},
            "income": {"æ„ä¹‰": "ç”¨æˆ·çš„å¹´æ”¶å…¥ï¼ŒæŒ‡ç”¨æˆ·ä¸€å¹´å†…è·å¾—çš„æ€»æ”¶å…¥ï¼ŒåŒ…æ‹¬å·¥èµ„ã€å¥–é‡‘ã€æŠ•èµ„æ”¶ç›Šç­‰ï¼Œæ˜¯è¡¡é‡ç”¨æˆ·ç»æµèƒ½åŠ›å’Œæ¶ˆè´¹æ½œåŠ›çš„é‡è¦æŒ‡æ ‡ã€‚", "å•ä½": "å…ƒ"},
            "education_level": {"æ„ä¹‰": "ç”¨æˆ·çš„æ•™è‚²æ°´å¹³ï¼Œé€šå¸¸ç”¨æ•°å­—ç¼–ç è¡¨ç¤ºä¸åŒçš„æ•™è‚²ç¨‹åº¦ï¼ˆå¦‚1=é«˜ä¸­ï¼Œ2=æœ¬ç§‘ï¼Œ3=ç ”ç©¶ç”Ÿï¼‰ï¼Œåæ˜ ç”¨æˆ·çš„çŸ¥è¯†èƒŒæ™¯å’Œè®¤çŸ¥èƒ½åŠ›ã€‚", "å•ä½": "ç­‰çº§"},
            "satisfaction_score": {"æ„ä¹‰": "ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†ï¼Œé€šè¿‡é—®å·è°ƒæŸ¥æˆ–è¯„ä»·ç³»ç»Ÿæ”¶é›†çš„ç”¨æˆ·å¯¹äº§å“æˆ–æœåŠ¡çš„æ»¡æ„ç¨‹åº¦ï¼Œé€šå¸¸é‡‡ç”¨æå…‹ç‰¹é‡è¡¨è¿›è¡Œæµ‹é‡ã€‚", "å•ä½": "åˆ†"},
            "city_code": {"æ„ä¹‰": "åŸå¸‚ä»£ç ï¼Œç”¨ç®€çŸ­çš„å­—æ¯æˆ–æ•°å­—ç»„åˆæ ‡è¯†ç”¨æˆ·æ‰€åœ¨çš„åŸå¸‚ï¼Œä¾¿äºè¿›è¡Œåœ°åŸŸåˆ†æå’ŒåŒºåŸŸå¸‚åœºç ”ç©¶ã€‚", "å•ä½": "ä»£ç "},
            "has_car": {"æ„ä¹‰": "æ˜¯å¦æ‹¥æœ‰æ±½è½¦ï¼ŒäºŒå…ƒå˜é‡è¡¨ç¤ºç”¨æˆ·æ˜¯å¦æ‹¥æœ‰ç§å®¶è½¦ï¼ˆ1è¡¨ç¤ºæœ‰ï¼Œ0è¡¨ç¤ºæ— ï¼‰ï¼Œåæ˜ ç”¨æˆ·çš„ç»æµçŠ¶å†µå’Œç”Ÿæ´»æ–¹å¼ã€‚", "å•ä½": "å¸ƒå°”å€¼"},
            "monthly_expense": {"æ„ä¹‰": "æœˆåº¦æ”¯å‡ºï¼Œç”¨æˆ·æ¯æœˆçš„å¹³å‡æ¶ˆè´¹æ”¯å‡ºé‡‘é¢ï¼ŒåŒ…æ‹¬ç”Ÿæ´»å¿…éœ€å“ã€å¨±ä¹ã€äº¤é€šç­‰å„é¡¹å¼€æ”¯ï¼Œç”¨äºåˆ†ææ¶ˆè´¹è¡Œä¸ºæ¨¡å¼ã€‚", "å•ä½": "å…ƒ"},
            "user_id": {"æ„ä¹‰": "ç”¨æˆ·å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºåŒºåˆ†ä¸åŒç”¨æˆ·çš„æ•°å­—æˆ–å­—ç¬¦ä¸²ç¼–ç ï¼Œç¡®ä¿æ•°æ®è®°å½•çš„å”¯ä¸€æ€§å’Œå¯è¿½æº¯æ€§ã€‚", "å•ä½": "æ ‡è¯†ç¬¦"},
            "registration_date": {"æ„ä¹‰": "æ³¨å†Œæ—¥æœŸï¼Œç”¨æˆ·é¦–æ¬¡æ³¨å†Œè´¦æˆ·æˆ–æœåŠ¡çš„æ—¥æœŸï¼Œç”¨äºåˆ†æç”¨æˆ·ç”Ÿå‘½å‘¨æœŸå’Œç•™å­˜æƒ…å†µã€‚", "å•ä½": "æ—¥æœŸ"},
            "last_login": {"æ„ä¹‰": "æœ€åç™»å½•æ—¶é—´ï¼Œç”¨æˆ·æœ€è¿‘ä¸€æ¬¡è®¿é—®ç³»ç»Ÿæˆ–ä½¿ç”¨æœåŠ¡çš„æ—¶é—´æˆ³ï¼Œç”¨äºè¯„ä¼°ç”¨æˆ·æ´»è·ƒåº¦å’Œå‚ä¸åº¦ã€‚", "å•ä½": "æ—¶é—´æˆ³"}
        }
        
        # ä»promptä¸­æå–æ‰€æœ‰åˆ—åï¼ˆæŸ¥æ‰¾"- "åé¢çš„åˆ—åï¼‰
        import re
        column_matches = re.findall(r'- (\w+)', prompt)
        
        if column_matches:
            # æŒ‰ç…§åˆ—åé¡ºåºè¿”å›å¯¹åº”çš„ç»“æœ
            results = []
            for column_name in column_matches:
                if column_name in column_mappings:
                    results.append(column_mappings[column_name])
                else:
                    results.append({"æ„ä¹‰": "å¾…äººå·¥è¡¥å……è¯´æ˜", "å•ä½": "æ²¡æœ‰å•ä½"})
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªåˆ—åï¼Œè¿”å›å•ä¸ªå¯¹è±¡ï¼›å¤šä¸ªåˆ—åè¿”å›æ•°ç»„
            if len(results) == 1:
                return json.dumps(results[0], ensure_ascii=False)
            else:
                return json.dumps(results, ensure_ascii=False)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„åˆ—åï¼Œè¿”å›é»˜è®¤å“åº”
        return json.dumps({"æ„ä¹‰": "å¾…äººå·¥è¡¥å……è¯´æ˜", "å•ä½": "æ²¡æœ‰å•ä½"}, ensure_ascii=False)


def create_user_behavior_data():
    """åˆ›å»ºç”¨æˆ·è¡Œä¸ºæµ‹è¯•æ•°æ®"""
    data = {
        'user_id': [f'U{str(i).zfill(4)}' for i in range(1, 21)],
        'age': [25, 30, 35, 28, 32, 29, 31, 27, 33, 26, 24, 36, 29, 31, 28, 34, 27, 30, 32, 25],
        'income': [50000, 60000, 70000, 55000, 65000, 58000, 62000, 53000, 68000, 51000,
                  48000, 72000, 59000, 63000, 56000, 69000, 52000, 61000, 66000, 49000],
        'education_level': [1, 2, 3, 2, 3, 2, 3, 1, 3, 2, 1, 3, 2, 3, 2, 3, 1, 2, 3, 1],
        'satisfaction_score': [4.2, 3.8, 4.5, 4.0, 4.3, 3.9, 4.1, 3.7, 4.4, 4.0,
                              3.6, 4.6, 3.9, 4.2, 4.0, 4.4, 3.8, 4.1, 4.3, 3.7],
        'city_code': ['BJ', 'SH', 'GZ', 'SZ', 'HZ', 'NJ', 'WH', 'CD', 'XA', 'QD',
                     'TJ', 'DL', 'SY', 'JN', 'ZZ', 'WX', 'SZ', 'FS', 'DG', 'ZH'],
        'has_car': [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0],
        'monthly_expense': [3000, 3500, 4000, 3200, 3800, 3100, 3600, 2900, 3900, 3300,
                           2800, 4200, 3150, 3750, 3250, 4100, 2950, 3650, 3850, 2750],
        'registration_date': pd.date_range('2020-01-01', periods=20, freq='15D'),
        'last_login': pd.date_range('2024-01-01', periods=20, freq='2D')
    }
    
    df = pd.DataFrame(data)
    
    # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼ç”¨äºæ¼”ç¤ºæ•°æ®æ¸…æ´—
    df.loc[1, 'income'] = None
    df.loc[3, 'satisfaction_score'] = None
    df.loc[7, 'monthly_expense'] = None
    df.loc[12, 'age'] = None
    df.loc[15, 'education_level'] = None
    
    return df


def main():
    print("=== æ•´åˆçš„åˆ—åå¤„ç†å·¥ä½œæµç¨‹ç¤ºä¾‹ ===")
    print("ä¸‰æ­¥éª¤æµç¨‹ï¼š1.ç”Ÿæˆåˆ—åæ¨¡æ¿ â†’ 2.æ•°æ®å¤„ç† â†’ 3.å¤§æ¨¡å‹å¡«å……æ„ä¹‰")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("\nğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®...")
    df = create_user_behavior_data()
    print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"åˆ—å: {list(df.columns)}")
    
    # è®¾ç½®è¾“å‡ºç›®å½• - ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„output
    output_dir = os.path.join(str(project_root), "output", "integrated_column_processing_workflow")
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æµ‹è¯•æ•°æ®
    temp_csv_path = os.path.join(output_dir, "user_behavior_data.csv")
    df.to_csv(temp_csv_path, index=False, encoding='utf-8')
    
    # åˆ›å»ºå­˜å‚¨å¯¹è±¡
    storage = FileStorage(
        input_file_path=temp_csv_path,
        cache_path=output_dir,
        file_name_prefix="workflow"
    )
    
    print("\n" + "="*60)
    print("ğŸ”¸ ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆåˆ—åæ¨¡æ¿ (è¾“å‡ºå…¨éƒ¨åˆ—å)")
    print("="*60)
    
    # åˆ›å»ºæ¨¡æ¿ç”Ÿæˆå™¨
    template_generator = ColumnTemplateGeneratorOperator(
        storage=storage,
        template_format="standard",  # ä½¿ç”¨æ ‡å‡†æ ¼å¼ï¼ˆåªåŒ…å«æ„ä¹‰å’Œå•ä½ï¼‰
        output_filename="all_column_names_template.json"
    )
    
    # æ‰§è¡Œæ¨¡æ¿ç”Ÿæˆ
    template_result = template_generator.execute(df)
    
    print(f"âœ… åˆ—åæ¨¡æ¿ç”Ÿæˆå®Œæˆ")
    print(f"   - æ¨¡æ¿è·¯å¾„: {template_result['template_path']}")
    print(f"   - æ€»åˆ—æ•°: {template_result['total_columns']}")
    print(f"   - æ¨¡æ¿æ ¼å¼: {template_result['template_format']}")
    
    # æ˜¾ç¤ºç”Ÿæˆçš„æ¨¡æ¿å†…å®¹
    print("\nğŸ“„ ç”Ÿæˆçš„åˆ—åæ¨¡æ¿é¢„è§ˆ:")
    template_preview = dict(list(template_result['template'].items())[:5])  # åªæ˜¾ç¤ºå‰5ä¸ª
    for column, info in template_preview.items():
        print(f"   {column}: {info}")
    print("   ...")
    
    print("\n" + "="*60)
    print("ğŸ”¸ ç¬¬äºŒæ­¥ï¼šè¿›è¡Œæ•°æ®å¤„ç† (æ•°æ®æ¸…æ´—ã€å­˜å‚¨)")
    print("="*60)
    
    # åˆ›å»ºæ¨¡æ‹ŸLLMæœåŠ¡ç”¨äºæ•°æ®å¤„ç†
    llm_service = MockLLMService()
    
    # åˆ›å»ºæ•°æ®å¤„ç†ç®¡é“
    pipeline = Pipeline(storage=storage.step())  # åˆ›å»ºæ–°çš„å¤„ç†æ­¥éª¤
    
    # æ•°æ®åˆ—å¤„ç†æ“ä½œç¬¦
    data_process_op = DataColumnProcessOperator(
        dataset_name="ç”¨æˆ·è¡Œä¸ºåˆ†ææ•°æ®",
        dataset_description="åŒ…å«ç”¨æˆ·åŸºæœ¬ä¿¡æ¯ã€æ”¶å…¥ã€æ•™è‚²æ°´å¹³ã€æ»¡æ„åº¦è¯„åˆ†ã€åŸå¸‚ä»£ç ã€æ±½è½¦æ‹¥æœ‰æƒ…å†µã€æœˆåº¦æ”¯å‡ºã€æ³¨å†Œæ—¥æœŸå’Œæœ€åç™»å½•æ—¶é—´çš„ç”¨æˆ·è¡Œä¸ºæ•°æ®é›†ï¼Œç”¨äºåˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼å’Œç‰¹å¾ã€‚",
        db_connection_string=f"sqlite:///{output_dir}/user_behavior.db",
        service=llm_service
    )
    
    # æ‰§è¡Œæ•°æ®å¤„ç†
    try:
        # ç›´æ¥è¿è¡Œæ•°æ®å¤„ç†æ“ä½œç¬¦
        data_process_result = data_process_op.run(storage.step(), data=df)
        
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ")
        print(f"   - è¾“å‡ºè·¯å¾„: {data_process_result['output_path']}")
        print(f"   - æ•°æ®åº“è¡¨: {data_process_result['database_table']}")
        print(f"   - æœ€ç»ˆæ•°æ®å½¢çŠ¶: {data_process_result['final_data_shape']}")
        print(f"   - å¤„ç†çš„åˆ—æ•°: {data_process_result['processed_columns']}")
        
        # æ˜¾ç¤ºæ•°æ®æ¸…æ´—ç»Ÿè®¡
        if 'cleaning_stats' in data_process_result:
            stats = data_process_result['cleaning_stats']
            print(f"   - æ¸…æ´—ç»Ÿè®¡: ç¼ºå¤±å€¼å¡«å…… {stats.get('missing_filled', 0)} ä¸ª")
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†å¤±è´¥: {str(e)}")
        print("   ç»§ç»­æ‰§è¡Œç¬¬ä¸‰æ­¥...")
        data_process_result = {"output_path": "æ•°æ®å¤„ç†è·³è¿‡"}
    
    print("\n" + "="*60)
    print("ğŸ”¸ ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨å¤§æ¨¡å‹å¡«å……JSONæ„ä¹‰å­—æ®µ")
    print("="*60)
    
    # ä½¿ç”¨ä¿®æ”¹åçš„ColumnMeaningGeneratorOperatorè¿›è¡Œæ¨¡æ¿å¡«å……
    meaning_generator = ColumnMeaningGeneratorOperator(
        dataset_description="ç”¨æˆ·è¡Œä¸ºåˆ†ææ•°æ®é›†ï¼ŒåŒ…å«ç”¨æˆ·çš„äººå£ç»Ÿè®¡å­¦ä¿¡æ¯ã€ç»æµçŠ¶å†µã€æ»¡æ„åº¦è¯„ä»·å’Œè¡Œä¸ºæ•°æ®",
        template_mode=True,  # å¯ç”¨æ¨¡æ¿æ¨¡å¼
        service=llm_service
    )
    
    # æ‰§è¡Œæ„ä¹‰å¡«å……
    meaning_result = meaning_generator.run(
        storage=storage.step(),  # åˆ›å»ºæ–°çš„å¤„ç†æ­¥éª¤
        template_path=template_result['template_path']
    )
    
    print(f"âœ… æ„ä¹‰å¡«å……å®Œæˆ")
    print(f"   - è¾“å‡ºè·¯å¾„: {meaning_result['path']}")
    print(f"   - æ€»åˆ—æ•°: {meaning_result['total_columns']}")
    print(f"   - å·²å¡«å……åˆ—æ•°: {meaning_result['filled_columns']}")
    
    # ä¿å­˜æœ€ç»ˆçš„JSONæ–‡ä»¶
    final_json_path = os.path.join(output_dir, "final_column_meanings.json")
    
    # ä»ç»“æœä¸­æå–å¹¶è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼
    final_template = {}
    for column_info in meaning_result['column_meanings']['columns']:
        column_name = column_info['column_name']
        final_template[column_name] = {
            "æ„ä¹‰": column_info['meaning'],
            "å•ä½": column_info['unit']
        }
    
    # ä¿å­˜æœ€ç»ˆJSON
    with open(final_json_path, 'w', encoding='utf-8') as f:
        json.dump(final_template, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æœ€ç»ˆJSONå·²ä¿å­˜: {final_json_path}")
    
    print("\n" + "="*60)
    print("ğŸ“‹ æ•´åˆå·¥ä½œæµç¨‹å®Œæˆæ€»ç»“")
    print("="*60)
    
    print(f"\nğŸ¯ ä¸‰æ­¥éª¤å¤„ç†ç»“æœ:")
    print(f"   1ï¸âƒ£ åˆ—åæ¨¡æ¿: {template_result['template_path']}")
    print(f"   2ï¸âƒ£ æ•°æ®å¤„ç†: {data_process_result['output_path']}")
    print(f"   3ï¸âƒ£ æœ€ç»ˆJSON: {final_json_path}")
    
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   - æ€»åˆ—æ•°: {len(final_template)}")
    print(f"   - æ•°æ®è¡Œæ•°: {df.shape[0]}")
    print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœå¯¹æ¯”
    print(f"\nğŸ” å¤„ç†å‰åå¯¹æ¯”ï¼ˆå‰3ä¸ªåˆ—åï¼‰:")
    
    # è¯»å–åŸå§‹æ¨¡æ¿
    with open(template_result['template_path'], 'r', encoding='utf-8') as f:
        original_template = json.load(f)
    
    sample_columns = list(original_template.keys())[:3]
    
    for column in sample_columns:
        print(f"\n   åˆ—å: {column}")
        print(f"     ç¬¬ä¸€æ­¥æ¨¡æ¿: {original_template[column]}")
        print(f"     ç¬¬ä¸‰æ­¥å¡«å……: {final_template[column]}")
    
    print(f"\nğŸ‰ æ•´åˆçš„ä¸‰æ­¥éª¤å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆï¼")
    print(f"   æ»¡è¶³éœ€æ±‚ï¼šå…ˆè¾“å‡ºå…¨éƒ¨åˆ—å â†’ æ•°æ®å¤„ç† â†’ å¤§æ¨¡å‹å¡«å……JSON")


if __name__ == "__main__":
    main()